# -*- coding: utf-8 -*-
"""ML PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZBQVlUqXgRLIWX1N3fVAqyshMvZAESk

# **Laptop Price**

## Preprocessing
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler,PolynomialFeatures
from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error,r2_score
from scipy.stats import randint

data = pd.read_csv('/content/drive/MyDrive/PROJECT/ML/laptop_prices.csv')
data.head()

data.shape

data.duplicated().sum()

data.isnull().sum()

data.info()

data.columns

for i in data.columns:
    print(i,data[i].unique(),data[i].nunique())
    print('*'*100)

"""## Outliers"""

df=data[(data['Weight (kg)'] > 3.5) & (data['Weight (kg)'] < 1.2)]
df

df=data[(data['Battery Life (hours)'] > 11.5) & (data['Battery Life (hours)'] < 5.5)]
df

"""## Visualization"""

c = ['violet', 'plum', 'thistle', 'lavender','lightcoral','silver']  # Your list of colors

import seaborn as sns
sns.displot(data['Price ($)'])

plt.figure(figsize=(10, 4))
sns.boxplot(x='Brand', y='Price ($)', data=data)
plt.title('Price Distribution by Brand')
plt.show()

d= pd.DataFrame(data.groupby(['Processor'])['Price ($)'].mean())
d.sort_values(by='Price ($)', ascending=False,inplace=True)

plt.figure(figsize=(10, 4))

sns.barplot(x=d.index, y=d['Price ($)'], order=d.index)

plt.title("Processor VS AVG Price ($)")
plt.show()

plt.figure(figsize=(10, 4))
sns.lineplot(x='RAM (GB)', y='Price ($)', data=data)

plt.title("RAM (GB) VS Price ($)")
plt.show()

"""## Transformation"""

Brand_ohe = OneHotEncoder(sparse_output=False)
Brand_ohe.fit(data[['Brand']])
Brand = Brand_ohe.transform(data[['Brand']])
Br = pd.DataFrame(Brand,columns=Brand_ohe.get_feature_names_out())

import re
import pandas as pd

class DataTransformer:
    def __init__(self, data):
        self.data = data
        self.processor_mapping = {
            'AMD Ryzen 3': 1, 'Intel i3': 2, 'Intel i5': 3, 'AMD Ryzen 5': 4,
            'AMD Ryzen 7': 5, 'Intel i7': 6, 'AMD Ryzen 9': 7, 'Intel i9': 8
        }
        self.inverse_processor_mapping = {v: k for k, v in self.processor_mapping.items()}

        self.gpu_mapping = {
            "Integrated": 1, "Nvidia GTX 1650": 2, "Nvidia RTX 2060": 3,
            "AMD Radeon RX 6600": 4, "Nvidia RTX 3060": 5,
            "AMD Radeon RX 6800": 6, "Nvidia RTX 3080": 7
        }
        self.inverse_gpu_mapping = {v: k for k, v in self.gpu_mapping.items()}

    def transform(self):
        """Encodes the 'Processor', 'Storage', 'GPU', and 'Resolution' columns in the given DataFrame."""
        if 'Processor' in self.data.columns:
            self.data['Processor'] = self.data['Processor'].map(self.processor_mapping)

        if 'Storage' in self.data.columns:
            self.data[['Storage_Size', 'SSD', 'HDD']] = pd.DataFrame(
                self.convert_storage_to_ml_format(self.data['Storage'].tolist()), index=self.data.index
            )
            self.data.drop(columns=['Storage'], inplace=True)

        if 'GPU' in self.data.columns:
            self.data['GPU'] = self.data['GPU'].map(self.gpu_mapping)

        if 'Resolution' in self.data.columns:
            self.data['Resolution'] = self.data['Resolution'].apply(self.convert_resolution)

        return self.data

    def inverse_transform(self):
        """Decodes numerical values back to their original labels for 'Processor' and 'GPU', and reconstructs 'Storage'."""
        if 'Processor' in self.data.columns:
            self.data['Processor'] = self.data['Processor'].map(self.inverse_processor_mapping)

        if 'GPU' in self.data.columns:
            self.data['GPU'] = self.data['GPU'].map(self.inverse_gpu_mapping)

        if {'Storage_Size', 'SSD', 'HDD'}.issubset(self.data.columns):
            self.data['Storage'] = self.data.apply(lambda row: self.reconstruct_storage(row), axis=1)
            self.data.drop(columns=['Storage_Size', 'SSD', 'HDD'], inplace=True)

        return self.data

    def convert_storage_to_ml_format(self, storage_list):
        """Converts storage descriptions to a machine learning-compatible format."""
        data = []

        for item in storage_list:
            match = re.match(r"(\d+)(TB|GB)\s+(SSD|HDD)", item)
            if match:
                size, unit, storage_type = match.groups()
                size = int(size)

                if unit == "TB":
                    size *= 1024

                ssd = 1 if storage_type == "SSD" else 0
                hdd = 1 if storage_type == "HDD" else 0

                data.append([size, ssd, hdd])

        return data

    def reconstruct_storage(self, row):
        """Reconstructs the storage description from numerical format."""
        size = row['Storage_Size']
        storage_type = 'SSD' if row['SSD'] == 1 else 'HDD'
        unit = 'TB' if size >= 1024 else 'GB'
        size = size // 1024 if unit == 'TB' else size
        return f"{size}{unit} {storage_type}"

    def convert_resolution(self, resolution):
        """Converts resolution from string format '1920x1080' to integer (total pixels)."""
        a, b = map(int, resolution.split('x'))
        return a * b

DataTransforme = DataTransformer(data)

data = DataTransforme.transform()
data

data['Resolution'].unique()

Operating_System_ohe = OneHotEncoder(sparse_output=False)
Operating_System_ohe.fit(data[['Operating System']])
Operating_System = Operating_System_ohe.transform(data[['Operating System']])
os = pd.DataFrame(Operating_System,columns=Operating_System_ohe.get_feature_names_out())

data.drop(columns=['Brand','Operating System'],inplace=True)

d = pd.concat([data,Br,os],axis = 1)
d

"""## correlation"""

correlation = d.corr()
correlation

corr = correlation['Price ($)']
corr

selected_columns = ['Price ($)', 'RAM (GB)','Processor', 'Storage_Size', 'Weight (kg)','Screen Size (inch)', 'Battery Life (hours)','GPU','Resolution']
correlation = d[selected_columns].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='Blues',fmt='.2f', cbar=True, linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""## train test split"""

x = d.drop(columns=['Price ($)'])
y = d['Price ($)']

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.20,random_state=1)

scaler = MinMaxScaler()
scaler.fit(xtrain)
xtrain = scaler.transform(xtrain)
scaler.fit(xtest)
xtest = scaler.transform(xtest)

"""# **MODEL BUILDING**

## **KNN**
"""

clsa = KNeighborsRegressor()
clsa.fit(xtrain,ytrain)
ypred = clsa.predict(xtest)
ypred

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred))

"""### GridSearchCSV"""

cls1 = KNeighborsRegressor()

help(cls1)

params = {'n_neighbors':[3,5,7,9,11,13,15],'weights':['uniform','distance'],'metric':['euclidean','manhattan','minkowski'],'p':[1,2]}
clf = GridSearchCV(cls1,params,cv=10,scoring='r2')
clf.fit(xtrain,ytrain)

clf.best_params_

cls = KNeighborsRegressor(metric='manhattan',n_neighbors= 5, p=1, weights='distance')
cls.fit(xtrain,ytrain)
ypred_knn = cls.predict(xtest)
ypred_knn

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred_knn))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypred_knn)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""## **DecisionTree**"""

from sklearn.feature_selection import SelectKBest,f_regression
sk= SelectKBest(score_func=f_regression,k='all')
sk.fit(x,y)
sk.scores_

cor_data=pd.DataFrame({'col_name':x.columns,'score':sk.scores_})
cor_data

dta = DecisionTreeRegressor()
dta.fit(xtrain,ytrain)
ypred_dt = dta.predict(xtest)
ypred_dt

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred_dt))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypred_dt)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""## **Linear regression**"""

lr=LinearRegression()
lr.fit(xtrain,ytrain)
ypred=lr.predict(xtest)
ypred

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred))

print("slope",lr.coef_)
print("y intercept",lr.intercept_)

print('trainscore : ',lr.score(xtrain,ytrain))
print('testscore : ',lr.score(xtest,ytest))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypred)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""### polynomial"""

poly=PolynomialFeatures(degree=3)
xpolytr=poly.fit_transform(xtrain)
xpolytr

xpolyts=poly.fit_transform(xtest)
xpolyts

lin=LinearRegression()
lin.fit(xpolytr,ytrain)

ypoly=lin.predict(xpolyts)
ypoly

print('simple linear regression: ',r2_score(ytest,ypred))
print('polynomial regression: ',r2_score(ytest,ypoly))

print('trainscore : ',lin.score(xpolytr,ytrain))
print('testscore : ',lin.score(xpolyts,ytest))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypoly)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""## **SVM**"""

svr = SVR()
svr.fit(xtrain,ytrain)

ypred = svr.predict(xtest)
ypred

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred))

"""### RandomizedSearchCV"""

svra = SVR()

help(svra)

params = {'C': [0.1, 1, 10, 100], 'epsilon': [0.01, 0.1, 0.2, 0.5],'kernel': ['linear', 'rbf', 'poly'],'gamma': ['scale', 'auto', 0.1, 0.01],'degree': [2, 3, 4]}
rs = RandomizedSearchCV(svra, params, n_iter=50, cv=6, scoring='r2', random_state=42)
rs.fit(xtrain, ytrain)

rs.best_params_

svr = SVR(kernel = 'poly',gamma = 'scale',epsilon = 0.5, degree = 2, C = 100)
svr.fit(xtrain,ytrain)

ypred_svm = svr.predict(xtest)
ypred_svm

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred_svm))

"""## **Random Forest**"""

rf=RandomForestRegressor()
rf.fit(xtrain,ytrain)
ypred_rf=rf.predict(xtest)
ypred_rf

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred_rf))
print("r2 score : ",r2_score(ytest,ypred_rf))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypred_rf)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""## **Ada Boost**"""

adbst = AdaBoostRegressor(n_estimators=10,estimator=DecisionTreeRegressor(max_depth=6))
adbst.fit(xtrain, ytrain)
ypred_ad = adbst.predict(xtest)
ypred_ad

print('mean_absolute_error: ',mean_absolute_error(ytest,ypred))
print("r2 score : ",r2_score(ytest,ypred_ad))

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ytest, y=ypred_ad)
plt.xlabel("Actual Price ($)")
plt.ylabel("Predicted Price ($)")
plt.title("Actual vs. Predicted Laptop Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')
plt.show()

"""## **ACCURACY**"""

accuracy = pd.DataFrame({'MODEL': ["KNN", "Decition Tree", "Linear Regression", "SVM", "Random Forest", "Ada Boost"],'r2_score': [r2_score(ytest, ypred_knn), r2_score(ytest, ypred_dt), r2_score(ytest, ypoly),r2_score(ytest, ypred_svm), r2_score(ytest, ypred_rf), r2_score(ytest, ypred_ad)]},index=[1,2,3,4,5,6])
accuracy

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
plt.suptitle("Actual vs. Predicted Laptop Prices", fontsize=20)

axes[0, 0].scatter(x=ytest, y=ypred_knn,s=6)
axes[0, 0].set_xlabel("Actual Price ($)")
axes[0, 0].set_ylabel("Predicted Price ($)")
axes[0, 0].set_title("KNN")
axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

axes[0, 1].scatter(x=ytest, y=ypred_dt,s=6)
axes[0, 1].set_xlabel("Actual Price ($)")
axes[0, 1].set_ylabel("Predicted Price ($)")
axes[0, 1].set_title("Decition Tree")
axes[0, 1].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

axes[0, 2].scatter(x=ytest, y=ypoly,s=6)
axes[0, 2].set_xlabel("Actual Price ($)")
axes[0, 2].set_ylabel("Predicted Price ($)")
axes[0, 2].set_title("Linear Regression")
axes[0, 2].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

axes[1, 0].scatter(x=ytest, y=ypred_svm,s=6)
axes[1, 0].set_xlabel("Actual Price ($)")
axes[1, 0].set_ylabel("Predicted Price ($)")
axes[1, 0].set_title("SVM")
axes[1, 0].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

axes[1, 1].scatter(x=ytest, y=ypred_rf,s=6)
axes[1, 1].set_xlabel("Actual Price ($)")
axes[1, 1].set_ylabel("Predicted Price ($)")
axes[1, 1].set_title("Random Forest")
axes[1, 1].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

axes[1, 2].scatter(x=ytest, y=ypred_ad,s=6)
axes[1, 2].set_xlabel("Actual Price ($)")
axes[1, 2].set_ylabel("Predicted Price ($)")
axes[1, 2].set_title("Ada Boost")
axes[1, 2].plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

plt.tight_layout()
plt.show()

"""## **pickle**"""

import pickle
pickle.dump(rf,open('rfmodel.sav','wb'))

pickle.dump(scaler,open('rfscaler.sav','wb'))

pickle.dump(Brand_ohe,open('Brand.sav','wb'))
pickle.dump(Operating_System_ohe,open('Operating_System.sav','wb'))
pickle.dump(DataTransforme,open('DataTransformer.sav','wb'))